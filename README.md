В данной работе предлагается улучшение динамических архитектур для обучения с добавлением новых классов (class incremental learning),
основанное на анализе потока информации между последовательными стадиями обучения.

![TCIL architercture](https://raw.githubusercontent.com/YellowPancake/TCIL/main/pictures/TCIL.png)


Была модифицирована функция потерь в методе [TCIL](https://github.com/YellowPancake/TCIL), был введен регуляризатор, учитывающий специфические особенности обучения с добавлением новых классов.

$`L^{i}_{reg} = \sum_{F_i}\log\left(\sigma_{batch }\left(F_i\right ) \right )`$, $`F_i`$ - набор активаций i-го энкодера 

$`L = L_{TCIL} + w_{reg} * \left(-L^{n}_{reg} + \sum_{i=1..n-1} L^{i}_{reg}) \right )`$ , $`n`$ - номер текущей стадии обучения




Этот регуляризатор повышает энтропию для текущей задачи, одновременно уменьшая энтропию для предыдущих задач. Это способствует уменьшению эффекта катастрофического забывания и более точному различению между классами из разных шагов обучения.

Наш метод улучшает существующий state-of-the-art подход TCIL. В результате применения нашего подхода, экспериментально подтверждено улучшение точности на наборе данных CIFAR100.  В сравнении с текущими методами, наш подход демонстрирует превосходство в большинстве экспериментов, показывая улучшение как top-1, так и top-5 точности





## Результаты

### Total accuracy top1/top5


| pretrain <br/> classes | num finetune <br/> stages | OUR <br/>  $`w_{reg}=3\mathrm{e}{-5}`$ | TCIL            |
|------------------------|---------------------------|----------------------------------------|-----------------|
| 0                      | 5                         | **69.83**/91.6                         | 69.51/**92.17** |
| 0                      | 10                        | **66.38/90.18**                        | 65.79/89.99     |
| 0                      | 20                        | **63.26/87.6**                         | 62.47/86.95     |
|                        |                           |                                        |                 |
| 50                     | 2                         | **72.66/92.74**                        | 72.61/92.4      |
| 50                     | 5                         | **71.03/91.93**                        | 70.11/91.48     |
| 50                     | 10                        | **69.21/91.36**                        | 68.24/89.97     |



| exp_name \ w  | OUR  $`w_{reg}=3e-5`$  | TCIL            | 2e5           |
|---------------|------------------------|-----------------|---------------|
| cifar_b0_10s  | **66.38/90.18**        | 65.79/89.99     | 65.39/88.77   |
| cifar_b0_20s  | **63.26/87.6**         | 62.47/86.95     | 61.09/83.74   |
| cifar_b0_5s   | **69.83**/91.6         | 69.51/**92.17** | 69.5/91.77    |
|               |                        |                 |               |
| cifar_b50_10s | **69.21/91.36**        | 68.24/89.97     | 68.0/89.95    |
| cifar_b50_2s  | **72.66/92.74**        | 72.61/92.4      | 72.05/92.36   |
| cifar_b50_5s  | **71.03/91.93**        | 70.11/91.48     | 70.34/91.49   |

### Incremental accuracy top1/top5

| exp_name \ w  | -3e-5            | TCIL            | 2e5             | paper  |
|---------------|------------------|-----------------|-----------------|--------|
| cifar_b0_10s  | 76.25/94.31      | **76.80/94.33** | 76.625/94.05    | 77.30  |
| cifar_b0_20s  | **74.99/93.36**  | 74.37/93.20     | 73.24/92.18     | 75.11  |
| cifar_b0_5s   | 77.42/94.51      | 77.69/**94.88** | **77.72**/94.58 | 77.72  |                 
|               |                  |                 |                 |        |
| cifar_b50_10s | **75.47/93.82**  | 74.97/93.379    | 74.80/93.38     | 73.72  |
| cifar_b50_2s  | **76.83/94.25**  | **76.83**/94.06 | 76.44/94.02     | 76.42  |
| cifar_b50_5s  | **76.39/93.99**  | 76.17/93.85     | 75.81/93.77     | 74.88  |


### Incremental accuracy top1/top5

| exp_name      | OUR             | TCIL            |
|---------------|-----------------|-----------------|
| cifar_b0_10s  | 76.25/94.31     | **76.80/94.33** |
| cifar_b0_20s  | **74.99/93.36** | 74.37/93.20     |
| cifar_b0_5s   | 77.42/94.51     | 77.69/**94.88** |                 
|               |                 |                 |
| cifar_b50_10s | **75.47/93.82** | 74.97/93.379    |
| cifar_b50_2s  | **76.83/94.25** | **76.83**/94.06 |
| cifar_b50_5s  | **76.39/93.99** | 76.17/93.85     |
 



### total accuracy top1/top5

| exp_name \ w  | -3e-5            | 0               | 2e5           |
|---------------|------------------|-----------------|---------------|
| cifar_b0_10s  | **66.38/90.18**  | 65.79/89.99     | 65.39/88.77   |
| cifar_b0_20s  | **63.26/87.6**   | 62.47/86.95     | 61.09/83.74   |
| cifar_b0_5s   | **69.83**/91.6   | 69.51/**92.17** | 69.5/91.77    |
|               |                  |                 |               |
| cifar_b50_10s | **69.21/91.36**  | 68.24/89.97     | 68.0/89.95    |
| cifar_b50_2s  | **72.66/92.74**  | 72.61/92.4      | 72.05/92.36   |
| cifar_b50_5s  | **71.03/91.93**  | 70.11/91.48     | 70.34/91.49   |


В данной научной статье мы представляем модифицированный подход к обучению нейронных сетей для задачи пошагового изучения классов, основанный на анализе потока информации между задачами.

Мы модифицировали функцию потерь, введя регуляризатор, который повышает энтропию для текущей задачи, одновременно уменьшая энтропию для предыдущих задач. Это способствует лучшему удержанию ранее изученной информации и более точному различению между классами из разных шагов обучения.

В данной научной статье мы представляем модифицированный подход к обучению нейронных сетей для задачи пошагового изучения классов, основанный на анализе потока информации между задачами. Наш подход включает внедрение нового регуляризатора, который способствует более эффективному обучению моделей, минимизируя путаницу между задачами и улучшая результаты классификации.

Наши эксперименты на наборе данных CIFAR-100 демонстрируют, что предложенный метод превосходит существующие подходы по показателям точности классификации. В частности, мы наблюдаем улучшение точности для различных стратегий инкрементального обучения, что подтверждает эффективность нашего регуляризатора в минимизации путаницы между задачами и повышении общей производительности модели.
